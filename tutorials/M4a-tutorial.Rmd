---
title: "R Tutorial: Randomization inference"
output: 
  rmarkdown::html_document:
    fig_width: 6
    fig_height: 4
    mathjax: null
    theme: cerulean
    highlight: pygments
    self_contained: false
editor_options: 
  chunk_output_type: console
---

<style>
pre > code.sourceCode a {
  display: none;
}
pre {
  border-width: 0px;
}
code {
  border-width: 0px;
}
</style>


```{r, include = FALSE}
if (require(canvasapicore)) {
  knitr::opts_knit$set(upload.fun = function(file) {
    canvasapicore::load_token_and_domain()
  	file <- stringr::str_replace(file, "Dropbox (Erasmus Universiteit Rotterdam)", "Dropbox_RSM")
    resp <- cnvs::cnvs_upload(file, "/api/v1/folders/1191228/files")
    structure(resp$url, XML = resp)
  })
}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


Read along with this tutorial, and run the R code in RStudio as you go (copy and paste, or type directly into R). After you have completed this tutorial, mark this task as done by clicking on "Mark as done" in the bottom-right corner of this page.

Start by loading packages:

```{r setup}
library(ECICourse)
library(tidyverse)
```

The  data for this tutorial describe the same fictional field  experiment,  conducted by an educational software company in cooperation with a local university, that was described in the previous tutorial. To recap: Students in the same degree program were randomized into two  conditions before entering  a required, core course. In the  treatment condition, students received access to the educational software. In the control condition, students did not  gain access to the software.

To obtain the data describing this experiment, run the following code:

```{r}
fieldexp <- get_ECI_data('module-4-tutorial')
```

As in the previous tutorial, the variable `fieldexp` is a data frame with `r ncol(fieldexp)` columns and `r nrow(fieldexp)` rows. Each row describes a  student. The column  `D` indicates whether the student was assigned to the treatment condition (`D == 1`) or to the control condition (`D == 0`). The column `Y` reflects the student's grade at the end of the course.

Next, repeat a few preliminary steps to  get  (re-)acquainted with the data set. First, look at the first few observations:

```{r}
fieldexp
```

Next, notice that half the students were assigned to the treatment and half to the control condition:

```{r}
fieldexp %>%
  count(D)
```

And finally, look at the distribution of grades in the  two conditions:

```{r M4-dist-DY}
ggplot(fieldexp, aes(x = factor(D), y = Y)) +
  geom_jitter(width = .1, height = .1) +
  labs(x = 'D')
```

## Average treatment effect

We will estimate the average treatment effect (ATE) for this experiment as the difference in values of `Y` when `D == 1` versus when `D == 0`. These  values are:

```{r}
fieldexp %>%
  group_by(D) %>%
  summarise(Y_avg = mean(Y))
```

The code below: 1) calculates the average values of `Y` for the two values of `D`, 2) orders the output according to the value of `D`,  3) extracts the two values of `Y_avg`, and 4)  calculates their difference (subtracting the first value of `Y_avg` for `D == 0` from the second value of `Y_avg` for `D == 1`).  These  steps produce a single number representing the estimated average  treatment effect.

```{r}
fieldexp %>%
  group_by(D) %>%
  summarise(Y_avg = mean(Y), .groups = 'drop') %>%
  arrange(D) %>%
  getElement('Y_avg') %>%
  diff()
```

*Note:* the `.groups = 'drop'` argument in the call to `summarise` prevents the message `summarise() ungrouping output (override with .groups argument)`  from being printed.

Later, it will be  useful to have this calculation encapsulated into a single function, so we will create that function now:

```{r}
get_ATE <- function(df) {
	df %>%
		group_by(D) %>%
		summarise(Y = mean(Y), .groups = 'drop') %>%
		arrange(D) %>%
		getElement('Y') %>%
		diff()
}
ate <- 
  fieldexp %>%
  get_ATE()
ate
```


```{r echo = FALSE}
sate <- round(ate, 3)
```

## Randomization inference

Should we trust that the estimated ATE of `r sate` reflects a true difference in grades due to the software, or should we be concerned that this estimate has occurred simply as the result of random sampling variation? We will use randomization inference to address this question.

We start by noting that there are many ways the experiment could have  assigned the 320 students into two groups of 160 each. In  fact, there are 320! / (160! 160!) different ways to assign students  into these two groups.  That's a huge number.

```{r}
choose(320, 160)
```

This number is so large that R  cannot represent it exactly as an integer (it is almost 100 digits long). It is not feasible consider every one of these possible random assignments.

The idea behind randomization inference is this: We start  with a working  assumption that the sharp null hypothesis  is correct. That is,  we assume that the treatment effect is exactly 0  for *every student.* This is the same as assuming that the potential outcomes for each student under `D == 0` and `D == 1`  are  the same. This further implies that if the sharp null hypothesis is true, then Y(1) = Y(0), and both potential outcomes are equal to whatever values  of  `Y` occur in the data.  

If all of these assumptions (from the sharp null hypothesis) are correct, then the estimate of the ATE (`r sate`) differs from 0 due to the way students were assigned  to the treatment  and control groups. This leads to the question, if this estimate is due entirely to sampling variation, how "typical" is the estimate we have obtained? We cannot answer this question exactly, because the number of ways to randomize the students is so large. But we can approximate it. 

We'll build this approximation in  steps. Let's first consider a single, hypothetical assignment of students to treatment and control. This assignment differs from the one we obtained in the actual experiment, but it was equally likely to have occurred.

```{r}
set.seed(1)             # so your results match mine
new_D <- sample(rep(0:1, 160), 320, replace = FALSE)
new_D
sum(new_D == 1)
sum(new_D == 0)
```

If this particular random assignment had actually happened, and if the sharp null  hypothesis  is in fact true,  then we would have obtained the following estimate  of the ATE:

```{r}
fieldexp %>%
  mutate(D = new_D) %>%
  get_ATE()
```

Combining this operation into a single function:

```{r}
get_random_ATE_under_sharp_null <- function() {
	new_D <- sample(rep(0:1, 160), 320, replace = FALSE)
	fieldexp %>%
		mutate(D = new_D) %>%
		get_ATE()
}
```

Each call  to this function yields an estimate of the  ATE that, if the sharp null hypothesis were true, would be  equally likely as  the estimate we actually obtained.

```{r}
get_random_ATE_under_sharp_null()
get_random_ATE_under_sharp_null()
get_random_ATE_under_sharp_null()
```

If we  generate many such estimates of the ATE, how do they  compare to the estimate we actually obtained? To answer that question, we need to rerun this function many  times (we'll use 1000 for this  example):

```{r}
random_ATEs_under_sharp_null <-  
	rerun(1000, get_random_ATE_under_sharp_null()) %>%
	unlist()
```

We can now compare  the estimate of the ATE obtained from the experiment (`r sate`) to these simulated estimates generated under the  assumption  that  the sharp null hypothesis is true. 

```{r M4-dist-ATE-sharp-null}
ggplot(NULL) +
  stat_bin(aes(x = random_ATEs_under_sharp_null), bins = 50) +
  geom_vline(aes(xintercept = ate))
```

The vertical line represents the ATE obtained from the experiment, and the values in the histogram represent other estimates of the ATE that might have occurred if the null hypothesis is true. Only a few of the 1000 simulated ATEs are more positive than the one we  did obtain.  We can  calculate  this proportion exactly:

```{r}
p <- mean(ate < random_ATEs_under_sharp_null)
p
```

This proportion is a statistic  that we call a p-value. For this experiment, it is the estimated probability of estimating an ATE greater than the one we did obtain if the sharp null hypothesis is true. The p-value for this hypothesis, given the experiment we actually conducted, is approximately `r p`.



## Using the coin package

The code above is  meant to be instructive, so it has the dual responsibility of conducting a permutation test while explaining how a permutation test works.  Because permutation tests are important for interpreting the  results of randomized experiments, there are  R packages that can do it more efficiently, and with less code. 

One such package is called  `coin`. If you don't have this package already, you can install it now:

```{r}
if (!require(coin)) {
  install.packages('coin')
}
library(coin)
```

The following code runs a permutation test, generating a conceptually identical p-value to the one we generated  above.

```{r}
perm_test <- 
  independence_test(Y ~ D, 
                    alternative = "greater", 
                    distribution = "approximate", 
                    data = fieldexp)
pvalue(perm_test)
```

The result of this permutation test is shown above. Moreover, because the  permutation test uses  a random subset of all possible assignments to treatment and control, the p-value itself is subject to sampling variation. Hence the output above shows a confidence interval around the p-value (notice that, in this case, the p-value we simulated earlier falls within this range).




