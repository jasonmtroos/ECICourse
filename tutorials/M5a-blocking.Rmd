---
title: "R Tutorial: Regression estimators and blocking"
output: 
  rmarkdown::html_document:
    fig_width: 6
    fig_height: 4
    mathjax: null
    theme: cerulean
    highlight: pygments
    self_contained: false
editor_options: 
  chunk_output_type: console
---

<style>
pre > code.sourceCode a {
  display: none;
}
pre {
  border-width: 0px;
}
code {
  border-width: 0px;
}
</style>


```{r, include = FALSE}
if (require(canvasapicore)) {
  knitr::opts_knit$set(upload.fun = function(file) {
    canvasapicore::load_token_and_domain()
    resp <- cnvs::cnvs_upload("/api/v1/folders/607253/files", file)
    structure(resp$url, XML = resp)
  })
}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


Read along with this tutorial, and run the R code in RStudio as you go (copy and paste, or type directly into R). After you have completed this tutorial, mark this task as done by clicking on "Mark as done" in the top-right corner of this page.

Start by loading packages:

```{r setup, cache = FALSE}
library(ECICourse)
library(tidyverse)
```

Also load the DeclareDesign package:

```{r,  cache = FALSE}
library(DeclareDesign)
```

In this tutorial, we will simulate experiments with both  complete and block random assignment. And for each design, we  will  estimate the average treatment effect (ATE) using different-in-means and regression estimators. The focus will be on quantifying the differences in the variance of the estimators. We will not try to point to a particular assignment and estimation strategy as the "best," since there is no such universal "best" experiment. Rather, the goal is to demonstrate  how different assignment and estimation strategies can potentially affect the precision of the estimated ATE.

## The population, potential outcomes, and experimental sample

To help fix ideas, imagine we will conduct an experiment involving a sample of employees from a large company. The experiment will assign some employees to receive special job training, and measure its impact on job performance. There are two variables that are expected to correlate with job performance. One is a binary variable, `W`, indicating whether the employee received a bonus in the previous year. The other is their level of formal education, `X`.

First, we will create a population to work with and  define a random sample drawn from it. 

```{r}
population <- 
  declare_population(
    N = 10000, 
    W = draw_binary(prob = .5, N = N),             # bonus
    X = draw_ordered(x = rnorm(N, 1 + .5 * W, 2),  # formal education
                     breaks = c(0, 1, 1.5, 3.5)))
```

`W` (bonus) is a binary variable, and `X` (education) is an ordered discrete variable. Values of `X` are higher among  employees with `W = 1` and lower among employees with `W = 0`.

```{r M5-pop-XW}
ggplot(population(), aes(x = X, fill = factor(W))) +
  stat_count(position = 'dodge') +
  labs(fill = 'W')
```

The experiment will measure an outcome variable `Y` (job performance). The potential outcomes differ between treatment `Z = 1` (special job training) and control `Z = 0` (no training) by a constant amount. Moreover, the potential outcomes are correlated with `W` and `X`.


```{r}
potential_outcomes <-
  declare_potential_outcomes(
    Y_Z_0 = W + X + rnorm(N),
    Y_Z_1 = Y_Z_0 + 5)
```

The value of `Y_Z_0` (i.e., the potential outcome Y(0)) is a random normal variable with mean `W + X` and a standard deviation of 1. The value of `Y_Z_1` is  simply `Y_Z_0 + 5`.

```{r M5-pop-Y0}
(population + potential_outcomes) %>%
  draw_data() %>%
  ggplot(aes(x = factor(X), y = Y_Z_0, colour = factor(W))) +
  geom_point(position = position_jitterdodge(
    jitter.width = .1, jitter.height = 0, dodge.width = .25)) +
  labs(y = 'Y(0)', x = 'X', colour = 'W')
```

The experiment will attempt to measure  the  ATE, which in these simulations, is equal to 5. This is the increase in job performance after receiving the training.

```{r}
ate <- declare_estimand(ATE = mean(Y_Z_1 - Y_Z_0), label = 'ATE')
```

For the simulated experiments, we will always work with sample of `N = 200` drawn from this population.

```{r}
exp_sample <- declare_sampling(n = 200)
```

The remaining design  choices differ depending on the assignment and estimation strategies. So for now we will combine the previous steps into a single object  called `basic_design`.

```{r}
basic_design <- population + potential_outcomes + ate + exp_sample
```

## Randomization

The most straightforward randomization  strategy is complete randomization with  equal probability of assignment into treatment  and control:

```{r}
complete_assn <- 
  declare_assignment(prob = .5, label = 'Complete')
```

The other strategies we will consider are blocking by covariates: by `W`, by `X`, or by both `W` and `X` jointly. 

```{r}
block_W_assn <- 
  declare_assignment(prob = .5, 
                     blocks = factor(W), 
                     label = 'Block by W')
block_X_assn <- 
  declare_assignment(prob = .5, 
                     blocks = factor(X), 
                     label = 'Block by X')
block_WX_assn <- 
  declare_assignment(prob = .5, 
                     blocks = factor(W):factor(X), 
                     label = 'Block by W and X')
```

These different randomization strategies lead to different numbers of individuals assigned to treatment and control in each of the subgroups defined by values of `W` (bonus) and `X` (education). To illustrate, consider a simulated experiment based on the complete assignment approach. We can tabulate the number of individuals assigned to treatment or control (`Z`) at different levels of `X`:

```{r}
(basic_design + complete_assn) %>%
  draw_data() %>%
  count(X, Z)
```

Now compare this with the assignments that are obtained through block randomization on `X`:

```{r}
(basic_design + block_X_assn) %>%
  draw_data() %>%
  count(X, Z)
```

Within each block, the number of individuals assigned to treatment and control differs by at most 1. Similarly, under blocked assignment, the values of `X` and `W` within each of the two treatment groups should be more balanced than under complete randomization.

```{r}
(basic_design + complete_assn) %>%
  draw_data() %>%
  group_by(Z) %>%
  summarise(avg_X = mean(X),  avg_W = mean(W))
(basic_design + block_WX_assn) %>%
  draw_data() %>%
  group_by(Z) %>%
  summarise(avg_X = mean(X),  avg_W = mean(W))
```


##  Estimators

The  most straightforward estimator is the simple difference-in-means estimator. We can define such an estimator in DeclareDesign as:

```{r}
ate_dim <- declare_estimator(Y ~ Z,
                             estimand = ate, 
                             label = 'DIM')
```

In practice, there are two ways we might implement such an estimator. One is using the `mean` function, the other is using a linear regression *without* covariates. Consider the following simulated experiment:

```{r}
sim <- (basic_design + complete_assn + declare_reveal()) %>%
  draw_data() %>%
  as_tibble()
sim
```

Estimating the ATE using `mean` looks something like  this:

```{r}
sim %>%
  group_by(Z) %>% 
  summarise(mean_Y = mean(Y)) %>%
  pivot_wider(names_from = 'Z', values_from = 'mean_Y') %>%
  mutate(ATE = `1` - `0`) %>%
  getElement('ATE')
```

Estimating the ATE using linear regression looks something like this:

```{r}
lm(Y ~ Z, data = sim) %>%
  coefficients() %>%
  .['Z']
```

As you can  see, the two methods produce the same estimated ATE (the change in job performance due to the job training).

The other estimators we will consider are based on linear regression *with* covariates. There are three such estimators we will work with, corresponding with the three block randomization strategies defined earlier. 

```{r}
ate_reg_W  <- declare_estimator(Y ~ Z + W,
                                model = lm,
                                term = 'Z',
                                estimand = ate, 
                                label = 'reg Y ~ Z + W')
ate_reg_X  <- declare_estimator(Y ~ Z + X,
                                model = lm,
                                term = 'Z',
                                estimand = ate, 
                                label = 'reg Y ~ Z + X')
ate_reg_WX <- declare_estimator(Y ~ Z + W + X,
                                model = lm,
                                term = 'Z',
                                estimand = ate, 
                                label = 'reg Y ~ Z + W + X')
```

As the code above implies, in practice, we use `lm` to estimate the ATE when adjusting or controlling for covariates. For example:

```{r}
lm(Y ~ Z + W + X, data = sim) %>%
  coefficients() %>%
  .['Z']
```

## Comparing assignment strategies with the difference-in-means  estimator

For this experiment, we can simulate many experiments using each of the assignment strategies (complete or the three blocking strategies), and then estimate the ATE using the difference-in-means estimator. 

```{r}
complete_dim <- basic_design + complete_assn + declare_reveal() + ate_dim
block_W_dim <- basic_design + block_W_assn + declare_reveal() + ate_dim
block_X_dim <- basic_design + block_X_assn + declare_reveal() + ate_dim
block_WX_dim <- basic_design + block_WX_assn + declare_reveal() + ate_dim
comparison_dim <- 
  diagnose_designs(complete_dim, block_W_dim, 
                   block_X_dim, block_WX_dim, 
                   sims = 1000)
comparison_dim
```

The `SD Estimate` column gives the standard deviation of the estimated ATEs for each of the four assignment strategies. Compared to complete randomization, blocking by `W` (bonus) does not improve the estimate by much at all. Blocking by `X` (education) lowers the standard error by a more substantial amount,  and blocking by `X` and `W` lowers it  even further. We can visualize the dispersion of the estimated ATEs across all simulations in order to see these differences more clearly:

```{r M5-dim-compare}
comparison_dim$simulations_df %>%
  transmute(estimate, 
            design_label = 
              str_replace(design_label, '_dim', '') %>%
              factor() %>% fct_inorder())  %>%
  ggplot(aes(x = estimate, colour = design_label)) +
  stat_density(alpha = 0, position = 'identity') +
  labs(colour = 'Assignment', x = 'Estimated ATE',
       title = 'Estimates from DIM Estimators')
```

## Comparing estimators under complete randomization

Next, we will perform a similar analysis as above. This time, however, we will simulate all of the experiments using complete randomization and estimate the ATE using the four different estimators (difference-in-means and the three regressions with covariates).  

```{r}
# complete_dim was defined above
complete_reg_W <- basic_design + complete_assn + declare_reveal() + ate_reg_W
complete_reg_X <- basic_design + complete_assn + declare_reveal() + ate_reg_X
complete_reg_WX <- basic_design + complete_assn + declare_reveal() + ate_reg_WX
comparison_complete <- 
  diagnose_designs(complete_dim, complete_reg_W, 
                   complete_reg_X, complete_reg_WX, 
                   sims = 1000)
comparison_complete
```

The  pattern  of improvement parallels the previous analysis. Adding the covariate `W`  has almost no benefit, adding `X` has a more substantial  benefit, and adding `W` and `X` together yields a modest further improvement over just `X`. 

```{r M5-complete-compare}
comparison_complete$simulations_df %>%
  transmute(estimate, 
            design_label = 
              str_replace(design_label, 'complete_', '') %>%
              factor() %>% fct_inorder())  %>%
  ggplot(aes(x = estimate, colour = design_label)) +
  stat_density(alpha = 0, position = 'identity') +
  labs(colour = 'Estimator', x = 'Estimated ATE',
       title = 'Estimates Under Complete Assignment')
```


## Comparing estimators under block assignment

The motivation behind blocked assignment is to remove variation in `Y` (job performance) that is  due to variation in the blocking variable (e.g., bonus, education, or both). Once that variation is removed, the difference-in-means estimator should be as efficient as a regression estimator that controls for the blocking variable.  In other words, we can still use the regression estimator, but because the  coefficients for the  blocking variables will be very close to zero, there is no benefit over the difference-in-means  estimator. To illustrate,  compare the regression estimator (with covariates  `W` and `X`)  with the difference-in-means estimators, when both are  applied to an experiment that blocks on `W` and `X`.

```{r}
block_WX_reg_WX <- basic_design + block_WX_assn + declare_reveal() + ate_reg_WX
block_WX_dim <- basic_design + block_WX_assn + declare_reveal() + ate_dim
comparison_block_WX <- 
  diagnose_designs(block_WX_reg_WX, block_WX_dim, 
                   sims = 1000)
comparison_block_WX
```

There is almost no difference, as we can see visually:

```{r M5-block-WX}
comparison_block_WX$simulations_df %>%
  transmute(estimate, 
            design_label = 
              str_replace(design_label, 'block_WX_', '') %>%
              factor() %>% fct_inorder())  %>%
  ggplot(aes(x = estimate, colour = design_label)) +
  stat_density(alpha = 0, position = 'identity') +
  labs(colour = 'Estimator', x = 'Estimated ATE',
       title = 'Estimates Under Blocked Assignment by W and X')
```

However, even  if we block, a regression estimator that uses covariates that were not part of the  blocked assignment can still yield improvements. For example, if  we block by `W` (bonus),  the regression estimator that adjusts for `X` (education) still shows improvement over the difference-in-means estimator:

```{r M5-block-W}
# block_W_dim was defined above
block_W_reg_X <- basic_design + block_W_assn + declare_reveal() + ate_reg_X
comparison_block_W <- 
  diagnose_designs(block_W_dim, block_W_reg_X, 
                   sims = 1000)
comparison_block_W
comparison_block_W$simulations_df %>%
  transmute(estimate, 
            design_label = 
              str_replace(design_label, 'block_W_', '') %>%
              factor() %>% fct_inorder())  %>%
  ggplot(aes(x = estimate, colour = design_label)) +
  stat_density(alpha = 0, position = 'identity')  +
  labs(colour = 'Estimator', x = 'Estimated ATE',
       title = 'Estimates Under Blocked Assignment by W')
```




